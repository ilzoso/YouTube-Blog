{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6d8659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import ujson\n",
    "\n",
    "#lm = dspy.LM('openai/gpt-4o-mini')\n",
    "lm = dspy.LM('ollama_chat/gemma3', api_base='http://localhost:11434', api_key='', cache=False, temperature=0)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b850abf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What states require IAR CE?',\n",
       " 'response': 'As of March 2025: Arkansas, California, Colorado, Florida, Hawaii, Kentucky, Maryland, Michigan, Mississippi, Nevada, North Dakota, Oklahoma, Oregon, South Carolina, Tennessee, Vermont, Washington D.C., Wisconsin',\n",
       " 'gold_doc_ids': [2822, 2823]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"registration-v1.jsonl\") as f:\n",
    "    data = [ujson.loads(line) for line in f]\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd0a0a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'What will happen if I do not complete IAR CE?', 'response': 'CE is mandatory. Failure to complete your CE requirement will lead to your IAR registration becoming unrenewable.', 'gold_doc_ids': []}) (input_keys={'question'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [dspy.Example(**d).with_inputs('question') for d in data]\n",
    "# Let's pick an `example` here from the data.\n",
    "example = data[2]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76bb884f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.Random(0).shuffle(data)\n",
    "trainset, devset, testset = data[:8], data[8:10], data[10:1000]\n",
    "\n",
    "len(trainset), len(devset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0c31d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 documents. Will encode them below.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IAR stands for Investment Advisor Registration'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_characters = 10  # for truncating >99th percentile of documents\n",
    "topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
    "\n",
    "with open(\"iar-v1.jsonl\") as f:\n",
    "    corpus = [ujson.loads(line)['text'] for line in f]\n",
    "    print(f\"Loaded {len(corpus)} documents. Will encode them below.\")\n",
    "\n",
    "corpus[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e97ab0",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedParamsError",
     "evalue": "litellm.UnsupportedParamsError: Setting {'dimensions': 512} is not supported by ollama_chat. To drop it from the call, set `litellm.drop_params = True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedParamsError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#embedder = dspy.Embedder('openai/text-embedding-3-small', dimensions=512)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m embedder \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mEmbedder(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mollama_chat/nomic-embed-text\u001b[39m\u001b[38;5;124m'\u001b[39m, dimensions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m search \u001b[38;5;241m=\u001b[39m \u001b[43mdspy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrievers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopk_docs_to_retrieve\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sukhdev\\Documents\\code\\rag-poc\\venv-10\\lib\\site-packages\\dspy\\retrievers\\embeddings.py:26\u001b[0m, in \u001b[0;36mEmbeddings.__init__\u001b[1;34m(self, corpus, embedder, k, callbacks, cache, brute_force_threshold, normalize)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus \u001b[38;5;241m=\u001b[39m corpus\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize \u001b[38;5;241m=\u001b[39m normalize\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_embeddings) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_embeddings\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_faiss() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(corpus) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m brute_force_threshold \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sukhdev\\Documents\\code\\rag-poc\\venv-10\\lib\\site-packages\\dspy\\clients\\embedding.py:134\u001b[0m, in \u001b[0;36mEmbedder.__call__\u001b[1;34m(self, inputs, batch_size, caching, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m embeddings_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m input_batches:\n\u001b[1;32m--> 134\u001b[0m     embeddings_list\u001b[38;5;241m.\u001b[39mextend(compute_embeddings(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, batch, caching\u001b[38;5;241m=\u001b[39mcaching, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess(embeddings_list, is_single_input)\n",
      "File \u001b[1;32mc:\\Users\\Sukhdev\\Documents\\code\\rag-poc\\venv-10\\lib\\site-packages\\dspy\\clients\\cache.py:224\u001b[0m, in \u001b[0;36mrequest_cache.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_result\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# Otherwise, compute and store the result\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    225\u001b[0m cache\u001b[38;5;241m.\u001b[39mput(modified_request, result)\n\u001b[0;32m    227\u001b[0m cache\u001b[38;5;241m.\u001b[39mignored_args_for_cache_key \u001b[38;5;241m=\u001b[39m original_ignored_args_for_cache_key\n",
      "File \u001b[1;32mc:\\Users\\Sukhdev\\Documents\\code\\rag-poc\\venv-10\\lib\\site-packages\\dspy\\clients\\embedding.py:161\u001b[0m, in \u001b[0;36m_cached_compute_embeddings\u001b[1;34m(model, batch_inputs, caching, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;129m@request_cache\u001b[39m(ignored_args_for_cache_key\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_base\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_url\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_cached_compute_embeddings\u001b[39m(model, batch_inputs, caching\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compute_embeddings(model, batch_inputs, caching\u001b[38;5;241m=\u001b[39mcaching, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Sukhdev\\Documents\\code\\rag-poc\\venv-10\\lib\\site-packages\\dspy\\clients\\embedding.py:151\u001b[0m, in \u001b[0;36m_compute_embeddings\u001b[1;34m(model, batch_inputs, caching, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    150\u001b[0m     caching \u001b[38;5;241m=\u001b[39m caching \u001b[38;5;129;01mand\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m     embedding_response \u001b[38;5;241m=\u001b[39m litellm\u001b[38;5;241m.\u001b[39membedding(model\u001b[38;5;241m=\u001b[39mmodel, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mbatch_inputs, caching\u001b[38;5;241m=\u001b[39mcaching, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m embedding_response\u001b[38;5;241m.\u001b[39mdata]\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(model):\n",
      "File \u001b[1;32mc:\\Users\\Sukhdev\\Documents\\code\\rag-poc\\venv-10\\lib\\site-packages\\litellm\\utils.py:1247\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[0;32m   1244\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[0;32m   1245\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[0;32m   1246\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\Sukhdev\\Documents\\code\\rag-poc\\venv-10\\lib\\site-packages\\litellm\\utils.py:1125\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1123\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m-> 1125\u001b[0m result \u001b[38;5;241m=\u001b[39m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1126\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Sukhdev\\Documents\\code\\rag-poc\\venv-10\\lib\\site-packages\\litellm\\main.py:3379\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(model, input, dimensions, encoding_format, timeout, api_base, api_version, api_key, api_type, caching, user, custom_llm_provider, litellm_call_id, logger_fn, **kwargs)\u001b[0m\n\u001b[0;32m   3376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dynamic_api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3377\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m dynamic_api_key\n\u001b[1;32m-> 3379\u001b[0m optional_params \u001b[38;5;241m=\u001b[39m get_optional_params_embeddings(\n\u001b[0;32m   3380\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   3381\u001b[0m     user\u001b[38;5;241m=\u001b[39muser,\n\u001b[0;32m   3382\u001b[0m     dimensions\u001b[38;5;241m=\u001b[39mdimensions,\n\u001b[0;32m   3383\u001b[0m     encoding_format\u001b[38;5;241m=\u001b[39mencoding_format,\n\u001b[0;32m   3384\u001b[0m     custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m   3385\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnon_default_params,\n\u001b[0;32m   3386\u001b[0m )\n\u001b[0;32m   3388\u001b[0m \u001b[38;5;66;03m### REGISTER CUSTOM MODEL PRICING -- IF GIVEN ###\u001b[39;00m\n\u001b[0;32m   3389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_cost_per_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m output_cost_per_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Sukhdev\\Documents\\code\\rag-poc\\venv-10\\lib\\site-packages\\litellm\\utils.py:2764\u001b[0m, in \u001b[0;36mget_optional_params_embeddings\u001b[1;34m(model, user, encoding_format, dimensions, custom_llm_provider, drop_params, additional_drop_params, **kwargs)\u001b[0m\n\u001b[0;32m   2762\u001b[0m                 non_default_params\u001b[38;5;241m.\u001b[39mpop(k, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   2763\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2764\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m UnsupportedParamsError(\n\u001b[0;32m   2765\u001b[0m                 status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[0;32m   2766\u001b[0m                 message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnon_default_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_llm_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. To drop it from the call, set `litellm.drop_params = True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2767\u001b[0m             )\n\u001b[0;32m   2768\u001b[0m final_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnon_default_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m   2769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_params\n",
      "\u001b[1;31mUnsupportedParamsError\u001b[0m: litellm.UnsupportedParamsError: Setting {'dimensions': 512} is not supported by ollama_chat. To drop it from the call, set `litellm.drop_params = True`."
     ]
    }
   ],
   "source": [
    "\n",
    "#embedder = dspy.Embedder('openai/text-embedding-3-small', dimensions=512)\n",
    "embedder = dspy.Embedder('ollama/nomic-embed-text', dimensions=512)\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66b65147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_docs=5):\n",
    "        self.num_docs = num_docs\n",
    "        self.respond = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = search(question, k=self.num_docs)   # not defined in this snippet, see link above\n",
    "        return self.respond(context=context, question=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee2bdf13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m rag \u001b[38;5;241m=\u001b[39m RAG()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat are high memory and low memory on linux?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sukhdev\\Documents\\code\\rag-poc\\venv-10\\lib\\site-packages\\dspy\\utils\\callback.py:326\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.sync_wrapper\u001b[1;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m _get_active_callbacks(instance)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    328\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n\u001b[0;32m    330\u001b[0m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Sukhdev\\Documents\\code\\rag-poc\\venv-10\\lib\\site-packages\\dspy\\primitives\\program.py:32\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m         output\u001b[38;5;241m.\u001b[39mset_lm_usage(usage_tracker\u001b[38;5;241m.\u001b[39mget_total_tokens())\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m, in \u001b[0;36mRAG.forward\u001b[1;34m(self, question)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, question):\n\u001b[1;32m----> 7\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m(question, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs)   \u001b[38;5;66;03m# not defined in this snippet, see link above\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrespond(context\u001b[38;5;241m=\u001b[39mcontext, question\u001b[38;5;241m=\u001b[39mquestion)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'search' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "rag = RAG()\n",
    "rag(question=\"what are high memory and low memory on linux?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d46670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "qa = dspy.Predict('question: str -> response: str')\n",
    "response = qa(question=\"what are high memory and low memory on linux?\")\n",
    "\n",
    "print(response.response)\n",
    "\n",
    "cot = dspy.ChainOfThought('question -> response')\n",
    "cot(question=\"should curly braces appear on their own line?\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evaluate(RAG())\n",
    "\n",
    "tp = dspy.MIPROv2(metric=dspy.SemanticF1(), auto=\"medium\", num_threads=24)\n",
    "optimized_rag = tp.compile(RAG(), trainset=trainset, max_bootstrapped_demos=2, max_labeled_demos=2, requires_permission_to_run=False)\n",
    "\n",
    "pred = optimized_rag(question=\"cmd+tab does not work on hidden or minimized windows\")\n",
    "print(pred.response)\n",
    "\n",
    "evaluate(optimized_rag)\n",
    "\n",
    "optimized_rag.save(\"optimized_rag.json\")\n",
    "\n",
    "loaded_rag = RAG()\n",
    "loaded_rag.load(\"optimized_rag.json\")\n",
    "\n",
    "loaded_rag(question=\"cmd+tab does not work on hidden or minimized windows\")\n",
    "\n",
    "\n",
    "\n",
    "def load_conll_dataset() -> dict:\n",
    "    \"\"\"\n",
    "    Loads the CoNLL-2003 dataset into train, validation, and test splits.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dataset splits with keys 'train', 'validation', and 'test'.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # Use a temporary Hugging Face cache directory for compatibility with certain hosted notebook\n",
    "        # environments that don't support the default Hugging Face cache directory\n",
    "        os.environ[\"HF_DATASETS_CACHE\"] = temp_dir\n",
    "        return load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "\n",
    "def extract_people_entities(data_row: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts entities referring to people from a row of the CoNLL-2003 dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_row (Dict[str, Any]): A row from the dataset containing tokens and NER tags.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of tokens tagged as people.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        token\n",
    "        for token, ner_tag in zip(data_row[\"tokens\"], data_row[\"ner_tags\"])\n",
    "        if ner_tag in (1, 2)  # CoNLL entity codes 1 and 2 refer to people\n",
    "    ]\n",
    "\n",
    "def prepare_dataset(data_split, start: int, end: int) -> List[dspy.Example]:\n",
    "    \"\"\"\n",
    "    Prepares a sliced dataset split for use with DSPy.\n",
    "    \n",
    "    Args:\n",
    "        data_split: The dataset split (e.g., train or test).\n",
    "        start (int): Starting index of the slice.\n",
    "        end (int): Ending index of the slice.\n",
    "    \n",
    "    Returns:\n",
    "        List[dspy.Example]: List of DSPy Examples with tokens and expected labels.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        dspy.Example(\n",
    "            tokens=row[\"tokens\"],\n",
    "            expected_extracted_people=extract_people_entities(row)\n",
    "        ).with_inputs(\"tokens\")\n",
    "        for row in data_split.select(range(start, end))\n",
    "    ]\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_conll_dataset()\n",
    "\n",
    "# Prepare the training and test sets\n",
    "train_set = prepare_dataset(dataset[\"train\"], 0, 50)\n",
    "test_set = prepare_dataset(dataset[\"test\"], 0, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a51af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class PeopleExtraction(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Extract contiguous tokens referring to specific people, if any, from a list of string tokens.\n",
    "    Output a list of tokens. In other words, do not combine multiple tokens into a single value.\n",
    "    \"\"\"\n",
    "    tokens: list[str] = dspy.InputField(desc=\"tokenized text\")\n",
    "    extracted_people: list[str] = dspy.OutputField(desc=\"all tokens referring to specific people extracted from the tokenized text\")\n",
    "\n",
    "people_extractor = dspy.ChainOfThought(PeopleExtraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eedbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM('ollama_chat/mistral', api_base='http://localhost:11434', api_key='', cache=False, temperature=0)\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30658044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction_correctness_metric(example: dspy.Example, prediction: dspy.Prediction, trace=None) -> bool:\n",
    "    \"\"\"\n",
    "    Computes correctness of entity extraction predictions.\n",
    "    \n",
    "    Args:\n",
    "        example (dspy.Example): The dataset example containing expected people entities.\n",
    "        prediction (dspy.Prediction): The prediction from the DSPy people extraction program.\n",
    "        trace: Optional trace object for debugging.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if predictions match expectations, False otherwise.\n",
    "    \"\"\"\n",
    "    return prediction.extracted_people == example.expected_extracted_people\n",
    "\n",
    "evaluate_correctness = dspy.Evaluate(\n",
    "    devset=test_set,\n",
    "    metric=extraction_correctness_metric,\n",
    "    num_threads=24,\n",
    "    display_progress=True,\n",
    "    display_table=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f405cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_correctness(people_extractor, devset=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb3288",
   "metadata": {},
   "outputs": [],
   "source": [
    "mipro_optimizer = dspy.MIPROv2(\n",
    "    metric=extraction_correctness_metric,\n",
    "    auto=\"medium\",\n",
    ")\n",
    "optimized_people_extractor = mipro_optimizer.compile(\n",
    "    people_extractor,\n",
    "    trainset=train_set,\n",
    "    max_bootstrapped_demos=4,\n",
    "    requires_permission_to_run=False,\n",
    "    minibatch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_correctness(optimized_people_extractor, devset=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5148fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = sum([x['cost'] for x in lm.history if x['cost'] is not None])  # cost in USD, as calculated by LiteLLM for certain providers\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_people_extractor.save(\"optimized_extractor_001.json\")\n",
    "\n",
    "loaded_people_extractor = dspy.ChainOfThought(PeopleExtraction)\n",
    "loaded_people_extractor.load(\"optimized_extractor_001.json\")\n",
    "\n",
    "p1 = loaded_people_extractor(tokens=[\"Italy\", \"recalled\", \"Marcello\", \"Cuttitta\"]).extracted_people\n",
    "p2 = loaded_people_extractor(tokens=[\"Muralidhar\", \"Kamath\", \"went\", \"for\", \"a\", \"walk\", \"in\", \"Pittsburgh\"]).extracted_people\n",
    "print(p1, p2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
