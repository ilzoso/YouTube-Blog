{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2295f1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['anchor', 'positive', 'negative'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, SentenceTransformerTrainer, SentenceTransformerTrainingArguments, losses\n",
    "from sentence_transformers.sampler import BatchSampler\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.losses import TripletLoss\n",
    "from sentence_transformers.readers import LabelSentenceReader, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "trained_model_path = \"models/bert-base-nli-stsb-mean-tokens\"\n",
    "\n",
    "# Load pre-trained model - we are using the original Sentence-BERT for this example.\n",
    "sbert_model = SentenceTransformer('bert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "dataset = Dataset.from_json(\"registration-v1.jsonl\")\n",
    "#dataset = load_dataset(\"json\", data_files=\"registration-v1.jsonl\")\n",
    "dataset.shape\n",
    "# anchor   positive    negative\n",
    "dataset = dataset.rename_column(\"question\", \"anchor\")\n",
    "dataset = dataset.rename_column(\"response\", \"positive\")\n",
    "dataset = dataset.rename_column(\"nonanswer\", \"negative\")\n",
    "dataset\n",
    "train_dataset = dataset.select_columns([\"anchor\", \"positive\", \"negative\"]).take(7)\n",
    "eval_dataset = dataset.select_columns([\"anchor\", \"positive\", \"negative\"]).skip(7).take(4)\n",
    "train_dataset\n",
    "eval_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ecf94173",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    #output_dir=\"models/mpnet-base-all-nli-triplet\",\n",
    "    output_dir=trained_model_path,\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use \"in-batch negatives\" benefit from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    run_name=\"mpnet-base-all-nli-triplet\",  # Will be used in W&B if `wandb` is installed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cff11a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7509e209ae9a4777a363ab2f6b1aa475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 00:35, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.662100</td>\n",
       "      <td>0.689493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.076757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up data for fine-tuning \n",
    "#sentence_reader = LabelSentenceReader(folder='~/tsv_files')\n",
    "#data_list = sentence_reader.get_examples(filename='recipe_bot_data.tsv')\n",
    "#triplets = triplets_from_labeled_dataset(input_examples=data_list)\n",
    "#finetune_data = SentencesDataset(examples=triplets, model=sbert_model)\n",
    "#finetune_data = train_dataset\n",
    "#finetune_dataloader = DataLoader(finetune_data, shuffle=True, batch_size=16)\n",
    "loss = losses.TripletLoss(model=sbert_model)\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=sbert_model,\n",
    "    args = args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()\n",
    "sbert_model.save_pretrained(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce74400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2 = sbert_model\n",
    "new_question = \"\"\"Hello LCD, I have a question about IAR CE. What do I do?\"\"\"\n",
    "#model2 = SentenceTransformer(trained_model_path)\n",
    "model2 = SentenceTransformer.load(trained_model_path)\n",
    "encoded_question =model2.encode([new_question])\n",
    "print(len(encoded_question[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c20f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoded_question2 =sbert_model.encode([new_question])\n",
    "encoded_question2\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.similarity(encoded_question, encoded_question2)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3da5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_q1 = model2.encode([eval_dataset[0]['anchor']])\n",
    "encoded_q2 = model2.encode([eval_dataset[1]['anchor']])\n",
    "encoded_q3 = model2.encode([eval_dataset[2]['anchor']])\n",
    "encoded_q4 = model2.encode([eval_dataset[3]['anchor']])\n",
    "\n",
    "predictions2 = model2.similarity(encoded_question, encoded_q2)\n",
    "predictions2\n",
    "\n",
    "predictions2 = model2.similarity(encoded_question, encoded_q2)\n",
    "predictions2\n",
    "\n",
    "print(model2.similarity(encoded_question, encoded_q1))\n",
    "print(model2.similarity(encoded_question, encoded_q2))\n",
    "print(model2.similarity(encoded_question, encoded_q3))\n",
    "print(model2.similarity(encoded_question, encoded_q4))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
